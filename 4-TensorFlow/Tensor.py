import tensorflow as tf
import numpy as np
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # ä¸è¾“å‡ºInfo

'''ä¸€ã€åˆ›å»ºä¸€äº›åŸºæœ¬å¼ é‡'''
print('ä¸€ã€åˆ›å»ºä¸€äº›åŸºæœ¬å¼ é‡')
# 1.â€œæ ‡é‡â€ï¼ˆæˆ–ç§°â€œ0 ç§©â€å¼ é‡ï¼‰ã€‚æ ‡é‡åŒ…å«å•ä¸ªå€¼ï¼Œä½†æ²¡æœ‰â€œè½´â€ã€‚
rank_0_tensor = tf.constant(4)
print(rank_0_tensor)
# 2.â€œå‘é‡â€ï¼ˆæˆ–ç§°â€œ1 ç§©â€å¼ é‡ï¼‰å°±åƒä¸€ä¸ªå€¼åˆ—è¡¨ã€‚å‘é‡æœ‰ 1 ä¸ªè½´ï¼š
rank_1_tensor = tf.constant([2.0, 3.0, 4.0])
print(rank_1_tensor)
# 3.â€œçŸ©é˜µâ€ï¼ˆæˆ–ç§°â€œ2 ç§©â€å¼ é‡ï¼‰æœ‰ 2 ä¸ªè½´ï¼š
rank_2_tensor = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float16)
print(rank_2_tensor)
# 4.å¼ é‡çš„è½´å¯èƒ½æ›´å¤šï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªåŒ…å« 3 ä¸ªè½´çš„å¼ é‡ï¼š
rank_3_tensor = tf.constant([
    [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]],
    [[10, 11, 12, 13, 14], [15, 16, 17, 18, 19]],
    [[20, 21, 22, 23, 24], [25, 26, 27, 28, 29]]
])
print(rank_3_tensor)

'''äºŒã€å¼ é‡çš„è½¬æ¢'''
print('äºŒã€å¼ é‡çš„è½¬æ¢')
# é€šè¿‡ä½¿ç”¨ np.array æˆ– tensor.numpy æ–¹æ³•ï¼Œå°†å¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„ï¼š
arr1 = np.array(rank_2_tensor)
print(arr1)
print(type(arr1))
arr2 = rank_2_tensor.numpy()
print(arr2)
print(type(arr2))

'''ä¸‰ã€å¼ é‡çš„è¿ç®—'''
print('ä¸‰ã€å¼ é‡çš„è¿ç®—')
# 1.å¯ä»¥å¯¹å¼ é‡æ‰§è¡ŒåŸºæœ¬æ•°å­¦è¿ç®—
print('1.å¯ä»¥å¯¹å¼ é‡æ‰§è¡ŒåŸºæœ¬æ•°å­¦è¿ç®—:')
a = tf.constant([[1, 2],[3, 4]])
b = tf.ones([2,2],dtype=tf.int32) # å¿…é¡»ç±»å‹è¦ç›¸åŒ
print(tf.add(a, b), "\n") # åŠ æ³•
print(tf.multiply(a, b), "\n") # ç‚¹ä¹˜
print(tf.matmul(a, b), "\n") # çŸ©é˜µä¹˜æ³•
print(a + b, "\n") # åŠ æ³•
print(a * b, "\n") # ç‚¹ä¹˜
print(a @ b, "\n") # çŸ©é˜µä¹˜æ³•
# 2.å„ç§è¿ç®—éƒ½å¯ä»¥ä½¿ç”¨å¼ é‡ã€‚
print('2.å„ç§è¿ç®—éƒ½å¯ä»¥ä½¿ç”¨å¼ é‡:')
c = tf.constant([[4.0, 5.0], [10.0, 1.0]])
print(tf.reduce_max(c)) # æœ€å¤§å€¼
print(tf.math.argmax(c)) # æœ€å¤§å€¼ä¸‹è§’æ ‡
print(tf.nn.softmax(c)) # softmax
# 3.è½¬æ¢æˆå¼ é‡
print('3.è½¬æ¢æˆå¼ é‡:')
d = np.array([1,2,3])
print(tf.convert_to_tensor(d))
print(tf.reduce_max(d))
print(tf.reduce_max([1,2,3]))

'''å››ã€å¼ é‡å½¢çŠ¶ç®€ä»‹'''
# 4 ç§©å¼ é‡ï¼Œå½¢çŠ¶ï¼š[3, 2, 4, 5]
rank_4_tensor = tf.zeros([3, 2, 4, 5])
print(rank_4_tensor)
print("Type of every element:", rank_4_tensor.dtype) # æ•°æ®ç±»å‹
print("Number of axes:", rank_4_tensor.ndim) # ç§©4
print("Shape of tensor:", rank_4_tensor.shape) # å½¢çŠ¶(3, 2, 4, 5)
print("Elements along axis 0 of tensor:", rank_4_tensor.shape[0]) # ç¬¬ä¸€ä¸ªè½´3
print("Elements along the last axis of tensor:", rank_4_tensor.shape[-1]) # æœ€åçš„è½´
print("Total number of elements (3*2*4*5): ", tf.size(rank_4_tensor).numpy()) # å¤§å°
# Tensor.ndim å’Œ Tensor.shape ç‰¹æ€§ä¸è¿”å› Tensor å¯¹è±¡ã€‚
# å¦‚æœéœ€è¦ Tensorï¼Œè¯·ä½¿ç”¨ tf.rank æˆ– tf.shape å‡½æ•°ã€‚
print(tf.rank(rank_4_tensor)) # ç§©
print(tf.shape(rank_4_tensor)) # å½¢çŠ¶
# è™½ç„¶é€šå¸¸ç”¨ç´¢å¼•æ¥æŒ‡ä»£è½´ï¼Œä½†æ˜¯æ‚¨å§‹ç»ˆè¦è®°ä½æ¯ä¸ªè½´çš„å«ä¹‰ã€‚
# è½´ä¸€èˆ¬æŒ‰ç…§ä»å…¨å±€åˆ°å±€éƒ¨çš„é¡ºåºè¿›è¡Œæ’åºï¼šé¦–å…ˆæ˜¯æ‰¹æ¬¡è½´ï¼Œéšåæ˜¯ç©ºé—´ç»´åº¦ï¼ˆé•¿å®½ï¼‰ï¼Œæœ€åæ˜¯æ¯ä¸ªä½ç½®çš„ç‰¹å¾ã€‚
# è¿™æ ·ï¼Œåœ¨å†…å­˜ä¸­ï¼Œç‰¹å¾å‘é‡å°±ä¼šä½äºè¿ç»­çš„åŒºåŸŸã€‚

'''äº”ã€å¼ é‡ç´¢å¼•'''
# 1.å•è½´ç´¢å¼•
rank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34])
print(rank_1_tensor.numpy())
# ä½¿ç”¨æ ‡é‡ç¼–åˆ¶ç´¢å¼•ä¼šç§»é™¤è½´ï¼š
print("First:", rank_1_tensor[0].numpy())
print("Second:", rank_1_tensor[1].numpy())
print("Last:", rank_1_tensor[-1].numpy())
# ä½¿ç”¨ : åˆ‡ç‰‡ç¼–åˆ¶ç´¢å¼•ä¼šä¿ç•™è½´ï¼š
print("Everything:", rank_1_tensor[:].numpy())
print("Before 4:", rank_1_tensor[:4].numpy())
print("From 4 to the end:", rank_1_tensor[4:].numpy())
print("From 2, before 7:", rank_1_tensor[2:7].numpy())
print("Every other item:", rank_1_tensor[::2].numpy())
print("Reversed:", rank_1_tensor[::-1].numpy())

# 2.å¤šè½´ç´¢å¼•
# å¯¹äºé«˜ç§©å¼ é‡çš„æ¯ä¸ªå•ç‹¬çš„è½´ï¼Œéµå¾ªä¸å•è½´æƒ…å½¢å®Œå…¨ç›¸åŒçš„è§„åˆ™ã€‚
print(rank_2_tensor.numpy())
# ä¸ºæ¯ä¸ªç´¢å¼•ä¼ é€’ä¸€ä¸ªæ•´æ•°ï¼Œç»“æœæ˜¯ä¸€ä¸ªæ ‡é‡ã€‚
print(rank_2_tensor[1, 1].numpy())
# ä½¿ç”¨æ•´æ•°ä¸åˆ‡ç‰‡çš„ä»»æ„ç»„åˆç¼–åˆ¶ç´¢å¼•ï¼š
print("Second row:", rank_2_tensor[1, :].numpy())
print("Second column:", rank_2_tensor[:, 1].numpy())
print("Last row:", rank_2_tensor[-1, :].numpy())
print("First item in last column:", rank_2_tensor[0, -1].numpy())
print("Skip the first row:")
print(rank_2_tensor[1:, :].numpy(), "\n")
# 3è½´å¼ é‡çš„ç¤ºä¾‹
print(rank_3_tensor[:, :, 4])

'''# å…­ã€æ“ä½œå½¢çŠ¶'''
x = tf.constant([[1], [2], [3]])
print(x.shape)
print(x)
# å¯ä»¥ç›´æ¥è½¬æ¢ä¸ºlist
print(x.shape.as_list())
# é€šè¿‡é‡æ„å¯ä»¥æ”¹å˜å¼ é‡çš„å½¢çŠ¶ã€‚tf.reshape è¿ç®—çš„é€Ÿåº¦å¾ˆå¿«ï¼Œèµ„æºæ¶ˆè€—å¾ˆä½ï¼Œå› ä¸ºä¸éœ€è¦å¤åˆ¶åº•å±‚æ•°æ®ã€‚
reshaped = tf.reshape(x, [1, 3])
print(x.shape)
print(reshaped.shape)
# å±•å¹³å¼ é‡ï¼Œåˆ™å¯ä»¥çœ‹åˆ°å®ƒåœ¨å†…å­˜ä¸­çš„æ’åˆ—é¡ºåºã€‚
print(tf.reshape(rank_3_tensor, [-1]))
# ä¸€èˆ¬æ¥è¯´ï¼Œtf.reshape å”¯ä¸€åˆç†çš„ç”¨é€”æ˜¯ç”¨äºåˆå¹¶æˆ–æ‹†åˆ†ç›¸é‚»è½´ï¼ˆæˆ–æ·»åŠ /ç§»é™¤ 1ï¼‰ã€‚
# å¯¹äº 3x2x5 å¼ é‡ï¼Œé‡æ„ä¸º (3x2)x5 æˆ– 3x(2x5) éƒ½åˆç†ï¼Œå› ä¸ºåˆ‡ç‰‡ä¸ä¼šæ··æ·†ï¼š
print(tf.reshape(rank_3_tensor, [3*2, 5]), "\n")
print(tf.reshape(rank_3_tensor, [3, -1]))
print('------------------------------------------')

# é‡æ„å¯ä»¥å¤„ç†æ€»å…ƒç´ ä¸ªæ•°ç›¸åŒçš„ä»»ä½•æ–°å½¢çŠ¶ï¼Œä½†æ˜¯å¦‚æœä¸éµä»è½´çš„é¡ºåºï¼Œåˆ™ä¸ä¼šå‘æŒ¥ä»»ä½•ä½œç”¨ã€‚
# åˆ©ç”¨ tf.reshape æ— æ³•å®ç°è½´çš„äº¤æ¢;äº¤æ¢è½´ï¼Œæ‚¨éœ€è¦ä½¿ç”¨ tf.transpose
print(tf.reshape(rank_3_tensor, [2, 3, 5]), "\n")
# This is a mess
print(tf.reshape(rank_3_tensor, [5, 6]), "\n")
# This doesn't work at all
try:
  tf.reshape(rank_3_tensor, [7, -1])
except Exception as e:
  print(f"{type(e).__name__}: {e}")

'''ä¸ƒã€DTypes è¯¦è§£'''
print('ä¸ƒã€DTypes è¯¦è§£')
# ä½¿ç”¨ Tensor.dtype å±æ€§å¯ä»¥æ£€æŸ¥ tf.Tensor çš„æ•°æ®ç±»å‹ã€‚
# ä» Python å¯¹è±¡åˆ›å»º tf.Tensor æ—¶ï¼Œæ‚¨å¯ä»¥é€‰æ‹©æŒ‡å®šæ•°æ®ç±»å‹ã€‚
# å¦‚æœä¸æŒ‡å®šï¼ŒTensorFlow ä¼šé€‰æ‹©ä¸€ä¸ªå¯ä»¥è¡¨ç¤ºæ‚¨çš„æ•°æ®çš„æ•°æ®ç±»å‹ã€‚
# TensorFlow å°† Python æ•´æ•°è½¬æ¢ä¸º tf.int32ï¼Œ
# å°† Python æµ®ç‚¹æ•°è½¬æ¢ä¸º tf.float32ã€‚
# å¦å¤–ï¼Œå½“è½¬æ¢ä¸ºæ•°ç»„æ—¶ï¼ŒTensorFlow ä¼šé‡‡ç”¨ä¸ NumPy ç›¸åŒçš„è§„åˆ™ã€‚
the_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64)
the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16)
# è½¬æ¢ç±»å‹
the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8)
print(the_u8_tensor)

'''å…«ã€å¹¿æ’­'''
x = tf.constant([1, 2, 3])
y = tf.constant(2)
z = tf.constant([2, 2, 2])
# å®ç°ç»“æœç›¸åŒ
print(tf.multiply(x, 2))
print(x * y)
print(x * z)
# å®ç°ç»“æœç›¸åŒ
x = tf.reshape(x,[3,1])
y = tf.range(1, 5)
print(x, "\n")
print(y, "\n")
print(tf.multiply(x, y))
# åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå¹¿æ’­çš„æ—¶é—´å’Œç©ºé—´æ•ˆç‡æ›´é«˜ï¼Œå› ä¸ºå¹¿æ’­è¿ç®—ä¸ä¼šåœ¨å†…å­˜ä¸­å…·ä½“åŒ–æ‰©å±•çš„å¼ é‡ã€‚
# ä½¿ç”¨ tf.broadcast_to å¯ä»¥äº†è§£å¹¿æ’­çš„è¿ç®—æ–¹å¼ã€‚
print(tf.broadcast_to(tf.constant([1, 2, 3]), [3, 3]))

'''ä¹ã€ä¸è§„åˆ™å¼ é‡'''
# å¦‚æœå¼ é‡çš„æŸä¸ªè½´ä¸Šçš„å…ƒç´ ä¸ªæ•°å¯å˜ï¼Œåˆ™ç§°ä¸ºâ€œä¸è§„åˆ™â€å¼ é‡ã€‚
# å¯¹äºä¸è§„åˆ™æ•°æ®ï¼Œè¯·ä½¿ç”¨ tf.ragged.RaggedTensorã€‚
ragged_list = [[0, 1, 2, 3],[4, 5],[6, 7, 8],[9]]
try:
  tensor = tf.constant(ragged_list)
except Exception as e:
  print(f"{type(e).__name__}: {e}")
# åº”ä½¿ç”¨ tf.ragged.constant æ¥åˆ›å»º tf.RaggedTensorï¼š
ragged_tensor = tf.ragged.constant(ragged_list)
print(ragged_tensor)
# tf.RaggedTensor çš„å½¢çŠ¶å°†åŒ…å«ä¸€äº›å…·æœ‰æœªçŸ¥é•¿åº¦çš„è½´ï¼š
print(ragged_tensor.shape)

'''åã€å­—ç¬¦ä¸²å¼ é‡'''
print('åã€å­—ç¬¦ä¸²å¼ é‡')
# Tensors can be strings, too here is a scalar string.
scalar_string_tensor = tf.constant("Gray wolf")
print(scalar_string_tensor)
# If you have three string tensors of different lengths, this is OK.
tensor_of_strings = tf.constant(["Gray wolf",
                                 "Quick brown fox",
                                 "Lazy dog"])
# Note that the shape is (3,). The string length is not included.
# åœ¨ Python3 ä¸­ï¼Œå­—ç¬¦ä¸²è¢«è§†ä¸º Unicode å­—ç¬¦ä¸²ï¼Œå³æ¯ä¸ªå­—ç¬¦éƒ½æ˜¯ Unicode ç¼–ç ã€‚
# è€Œåœ¨ TensorFlow ä¸­ï¼Œå­—ç¬¦ä¸²ç±»å‹çš„å¼ é‡æ˜¯ä½¿ç”¨å­—èŠ‚å­—ç¬¦ä¸²è¡¨ç¤ºï¼ˆByte Stringï¼‰ï¼Œ
# å³æ¯ä¸ªå­—ç¬¦æ˜¯ 8 æ¯”ç‰¹çš„å­—èŠ‚è¡¨ç¤ºã€‚å› æ­¤ï¼Œå½“è¾“å‡ºå­—ç¬¦ä¸²ç±»å‹çš„å¼ é‡æ—¶ï¼Œ
# éœ€è¦ä»¥å­—èŠ‚å­—ç¬¦ä¸²çš„æ ¼å¼è¾“å‡ºï¼Œè¿™æ—¶å°±ä¼šåœ¨å­—ç¬¦ä¸²å‰é¢åŠ ä¸Š "b" æ ‡è¯†å­—èŠ‚å­—ç¬¦ä¸²ã€‚
print(tensor_of_strings)
# å¦‚æœä¼ é€’ Unicode å­—ç¬¦ï¼Œåˆ™ä¼šä½¿ç”¨ utf-8 ç¼–ç ã€‚
print(tf.constant("ğŸ¥³ğŸ‘"))
# åœ¨ tf.strings ä¸­å¯ä»¥æ‰¾åˆ°ç”¨äºæ“ä½œå­—ç¬¦ä¸²çš„ä¸€äº›åŸºæœ¬å‡½æ•°ï¼ŒåŒ…æ‹¬ tf.strings.splitã€‚
print(tf.strings.split(scalar_string_tensor, sep=" "))
# ...but it turns into a `RaggedTensor` if you split up a tensor of strings,
# as each string might be split into a different number of parts.
print(tf.strings.split(tensor_of_strings))
# ä»¥åŠ tf.string.to_numberï¼š
text = tf.constant("1 10 100")
print(tf.strings.to_number(tf.strings.split(text, " ")))
# è™½ç„¶ä¸èƒ½ä½¿ç”¨ tf.cast å°†å­—ç¬¦ä¸²å¼ é‡è½¬æ¢ä¸ºæ•°å€¼ï¼Œä½†æ˜¯å¯ä»¥å…ˆå°†å…¶è½¬æ¢ä¸ºå­—èŠ‚ï¼Œç„¶åè½¬æ¢ä¸ºæ•°å€¼ã€‚
# tf.io æ¨¡å—åŒ…å«åœ¨æ•°æ®ä¸å­—èŠ‚ç±»å‹ä¹‹é—´è¿›è¡Œç›¸äº’è½¬æ¢çš„å‡½æ•°ï¼ŒåŒ…æ‹¬è§£ç å›¾åƒå’Œè§£æ csv çš„å‡½æ•°ã€‚
byte_strings = tf.strings.bytes_split(tf.constant("Duck"))
byte_ints = tf.io.decode_raw(tf.constant("Duck"), tf.uint8)
print("Byte strings:", byte_strings)
print("Bytes:", byte_ints)
# å…³äºUnicodeç±»å‹å­—ç¬¦çš„åˆ†å‰²ä¸è§£ç 
unicode_bytes = tf.constant("ã‚¢ãƒ’ãƒ« ğŸ¦†")
unicode_char_bytes = tf.strings.unicode_split(unicode_bytes, "UTF-8")
unicode_values = tf.strings.unicode_decode(unicode_bytes, "UTF-8")
print("\nUnicode bytes:", unicode_bytes)
print("\nUnicode chars:", unicode_char_bytes)
print("\nUnicode values:", unicode_values)

'''åä¸€ã€ç¨€ç–å¼ é‡'''
# åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ•°æ®å¾ˆç¨€ç–ï¼Œæ¯”å¦‚è¯´åœ¨ä¸€ä¸ªéå¸¸å®½çš„åµŒå…¥ç©ºé—´ä¸­ã€‚
# ä¸ºäº†é«˜æ•ˆå­˜å‚¨ç¨€ç–æ•°æ®ï¼ŒTensorFlow æ”¯æŒ tf.sparse.SparseTensor å’Œç›¸å…³è¿ç®—ã€‚
# Sparse tensors store values by index in a memory-efficient manner
sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]],
                                       values=[1, 2],
                                       dense_shape=[3, 4])
print(sparse_tensor, "\n")
# You can convert sparse tensors to dense
print(tf.sparse.to_dense(sparse_tensor))






